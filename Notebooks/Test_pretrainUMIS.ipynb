{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503eca3d-5086-43fe-86bb-bec67027a48a",
   "metadata": {},
   "source": [
    "## Check all dependencies are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f812c84-2eca-4fc4-9606-786e59824771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/papiez/users/hri611/python/UMIS\n"
     ]
    }
   ],
   "source": [
    "cd UMIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf39aeae-6678-403b-940b-1d7cba3e4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import random\n",
    "import logging\n",
    "import onnx\n",
    "import numpy as np\n",
    "import time\n",
    "import setproctitle\n",
    "import torch\n",
    "import torch.optim\n",
    "# from models import criterions\n",
    "from models.lib.VNet3D import VNet\n",
    "from plot import loss_plot,metrics_plot\n",
    "from models.lib.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\n",
    "from models.lib.vit_seg_modeling import ViT\n",
    "from models.lib.probabilistic_unet import ProbabilisticUnet\n",
    "from models.lib.probabilistic_unet2D import ProbabilisticUnet2D\n",
    "from models.lib.VNet3D import VNet\n",
    "from models.lib.VNet2D import VNet2D\n",
    "from models.lib.TransU_zoo import Transformer_U\n",
    "from models.lib.UNet3DZoo import Unet,AttUnet,Unetdrop\n",
    "from models.lib.UNet2DZoo import Unet2D,AttUnet2D,resnet34_unet,Unet2Ddrop\n",
    "# removed import below (anissa)\n",
    "#from models.lib.GetPromptModel import build_promptmodel\n",
    "#from models.criterions import softmax_dice,FocalLoss,DiceLoss,DC_and_BCE_loss,SDiceLoss,get_soft_label\n",
    "from models.criterions import softmax_dice,FocalLoss,DiceLoss,SDiceLoss,get_soft_label\n",
    "from data.Autopet2023 import Autopet\n",
    "from data.transform import ISIC2018_transform,LiTS2017_transform\n",
    "from data.BraTS2019 import BraTS\n",
    "from data.ISIC2018 import ISIC\n",
    "#from data.COVID19 import Covid\n",
    "#from data.CHAOS20 import CHAOS\n",
    "from data.LiTS17 import LiTS\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from models.lib.utils import l2_regularisation\n",
    "from tensorboardX import SummaryWriter\n",
    "from predict import validate_softmax,test_softmax,one_hot,one_hot_co,one_hot_co2D,testensemblemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14497cc-7e62-4ea5-944e-c8e237428ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.Autopet2023 import Autopet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb2e2f6-75c2-46cf-abc2-f1d4694a2092",
   "metadata": {},
   "source": [
    "## Define functions in pretrainUMIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f138b7dd-29f3-4ede-88ff-e5429f8d5ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getArgs():\n",
    "    local_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # anissa changes:\n",
    "    parser.add_argument('--user', default='anissa', type=str)\n",
    "    parser.add_argument('--experiment', default='UMIS', type=str)\n",
    "    parser.add_argument('--date', default=local_time.split(' ')[0], type=str)\n",
    "    parser.add_argument('--description',\n",
    "                        default='Trustworthy medical image segmentation by coco,'\n",
    "                                'training on trai_imgsn.txt!',\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--mode\", default=\"train\", type=str, help=\"train/test/train&test\")\n",
    "    parser.add_argument('--dataset', default='autopet', type=str, help=\"autopet/BraTS/ISIC/COVID/CHAOS/LiTS\")\n",
    "    parser.add_argument('--crop_H', default=128, type=int)\n",
    "    parser.add_argument('--crop_W', default=128, type=int)\n",
    "    parser.add_argument('--crop_D', default=128, type=int)\n",
    "    parser.add_argument('--num_classes', default=2, type=int)\n",
    "    parser.add_argument('--input_modality', default='petct', type=str, help=\"petct/t1/t2/both/four\")\n",
    "\n",
    "    \n",
    "    parser.add_argument(\"--folder\", default=\"folder0\", type=str, help=\"folder0/folder1/folder2/folder3/folder4\")\n",
    "    parser.add_argument('--input_C', default=4, type=int)\n",
    "    parser.add_argument('--input_H', default=240, type=int)\n",
    "    parser.add_argument('--input_W', default=240, type=int)\n",
    "    parser.add_argument('--input_D', default=160, type=int)  # 155\n",
    "    parser.add_argument('--output_D', default=155, type=int)\n",
    "    # Training Information\n",
    "    parser.add_argument('--lr', default=0.0001, type=float)\n",
    "    parser.add_argument('--weight_decay', default=1e-5, type=float)\n",
    "    # parser.add_argument('--amsgrad', default=True, type=bool)\n",
    "    parser.add_argument('--submission', default='./results', type=str)\n",
    "    parser.add_argument('--seed', default=1000, type=int)\n",
    "    parser.add_argument('--no_cuda', default=False, type=bool)\n",
    "    parser.add_argument('--batch_size', default=2, type=int, help=\"2/4/8/16\")\n",
    "    parser.add_argument('--start_epoch', default=0, type=int)\n",
    "    parser.add_argument('--end_epochs', default=200, type=int)\n",
    "    parser.add_argument('--save_freq', default=5, type=int)\n",
    "    parser.add_argument('--resume', default='', type=str)\n",
    "    parser.add_argument('--load', default=True, type=bool)\n",
    "    parser.add_argument('--model_name', default='AU', type=str, help=\"AU/V/U/PU/ResU/Udrop/UE0/\")\n",
    "    parser.add_argument('--en_time', default=10, type=int)\n",
    "    parser.add_argument('--OOD_Condition', default='normal', type=str, help=\"normal/noise/mask/blur/spike/ghost/\")\n",
    "    parser.add_argument('--OOD_Level', default=1, type=int, help=\"0: 'No',1:'Low', 2:'Upper Low', 3:'Mid', 4:'Upper Mid', 5:'High'\")\n",
    "    parser.add_argument('--use_TTA', default=False, type=bool, help=\"True/False\")\n",
    "    parser.add_argument('--snapshot', default=True, type=bool, help=\"True/False\") # visualization results\n",
    "    parser.add_argument('--save_format', default='nii', type=str)\n",
    "    parser.add_argument('--test_date', default='2023-01-01', type=str)\n",
    "    parser.add_argument('--test_epoch', default=199, type=int)\n",
    "    parser.add_argument('--n_skip', type=int,\n",
    "                        default=3, help='using number of skip-connect, default is num')\n",
    "    parser.add_argument('--vit_name', type=str,\n",
    "                        default='R50-ViT-B_16', help='select one vit model')\n",
    "    parser.add_argument('--vit_patches_size', type=int,\n",
    "                        default=16, help='vit_patches_size, default is 16')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "# added for jupyter astest\n",
    "def create_args(user='anissa', experiment='UMIS', date=None, description='Trustworthy medical image segmentation by coco, training on trai_imgsn.txt!',\n",
    "                mode='train', dataset='autopet', crop_H=128, crop_W=128, crop_D=128, num_classes=2,\n",
    "                input_modality='petct', folder='folder0', input_C=4, input_H=240, input_W=240, input_D=160,\n",
    "                output_D=155, lr=0.0001, weight_decay=1e-5, submission='./results', seed=1000, no_cuda=False,\n",
    "                batch_size=2, start_epoch=0, end_epochs=200, save_freq=5, resume='', load=True, model_name='AU',\n",
    "                en_time=10, OOD_Condition='normal', OOD_Level=1, use_TTA=False, snapshot=True, save_format='nii',\n",
    "                test_date='2023-01-01', test_epoch=199, n_skip=3, vit_name='R50-ViT-B_16', vit_patches_size=16):\n",
    "    local_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "    date = local_time.split(' ')[0] if date is None else date\n",
    "\n",
    "    args = argparse.Namespace(user=user, experiment=experiment, date=date, description=description,\n",
    "                              mode=mode, dataset=dataset, crop_H=crop_H, crop_W=crop_W, crop_D=crop_D,\n",
    "                              num_classes=num_classes, input_modality=input_modality, folder=folder,\n",
    "                              input_C=input_C, input_H=input_H, input_W=input_W, input_D=input_D,\n",
    "                              output_D=output_D, lr=lr, weight_decay=weight_decay, submission=submission,\n",
    "                              seed=seed, no_cuda=no_cuda, batch_size=batch_size, start_epoch=start_epoch,\n",
    "                              end_epochs=end_epochs, save_freq=save_freq, resume=resume, load=load,\n",
    "                              model_name=model_name, en_time=en_time, OOD_Condition=OOD_Condition,\n",
    "                              OOD_Level=OOD_Level, use_TTA=use_TTA, snapshot=snapshot, save_format=save_format,\n",
    "                              test_date=test_date, test_epoch=test_epoch, n_skip=n_skip, vit_name=vit_name,\n",
    "                              vit_patches_size=vit_patches_size)\n",
    "\n",
    "    return args\n",
    "\n",
    "def getModel(args):\n",
    "    if args.dataset == 'autopet':\n",
    "        model = Unet(in_channels=2, base_channels=16, num_classes=2) # changed num of channels and classes\n",
    "\n",
    "    # if args.dataset=='BraTS':\n",
    "    #     if args.model_name == 'TransU' and args.input_modality == 'four':\n",
    "    #         _, model = Transformer_U(dataset='BraTS', _conv_repr=True, _pe_type=\"learned\")\n",
    "    #     elif args.model_name == 'AU' and args.input_modality == 'four':\n",
    "    #         model = AttUnet(in_channels=4, base_channels=16, num_classes=args.num_classes)\n",
    "    #     elif args.model_name == 'AU':\n",
    "    #         model = AttUnet(in_channels=1, base_channels=16, num_classes=args.num_classes)\n",
    "    #     elif args.model_name == 'V' and args.input_modality == 'four':\n",
    "    #         model = VNet(n_channels=4, n_classes=args.num_classes, n_filters=16, normalization='gn', has_dropout=False)\n",
    "    #     elif args.model_name == 'UE0' or 'UE00' or 'UE01' or 'UE02' or 'UE03' or 'UE04' or 'UE05' or 'UE06' or 'UE07' or 'UE08' or 'UE09':\n",
    "    #         # args.lr = 0.0002 # O:0.0002  0.0001 0.0002 0.0003 0.001\n",
    "    #         # args.batch_size = 4  # O:8    4 8 16\n",
    "    #         model = Unet(in_channels=4, base_channels=16, num_classes=4)\n",
    "    #     elif args.model_name == 'U' and args.input_modality == 'four':\n",
    "    #         model = Unet(in_channels=4, base_channels=16, num_classes=4)\n",
    "    #     elif args.model_name == 'U':\n",
    "    #         model = Unet(in_channels=1, base_channels=16, num_classes=4)\n",
    "    \n",
    "    else:\n",
    "        print('There is no this dataset')\n",
    "        raise NameError\n",
    "    return model\n",
    "\n",
    "def getDataset(args):\n",
    "    if args.dataset =='autopet':\n",
    "        base_folder = args.folder\n",
    "\n",
    "        # anissa\n",
    "        root_path = '/gpfs3/well/papiez/users/hri611/python/UMIS/'\n",
    "        train_file = 'train_imgs.txt'\n",
    "        train_dir='train_data'\n",
    "        train_list = os.path.join(root_path, train_file)\n",
    "        train_root = os.path.join(root_path, train_dir)\n",
    "        train_set = Autopet(train_list, train_root, args.mode,args.input_modality,OOD_Condition=args.OOD_Condition, level=args.OOD_Level) # removed folder = base_folder\n",
    "        train_loader = DataLoader(dataset=train_set, batch_size=args.batch_size)\n",
    "        print('Samples for train = {}'.format(len(train_loader.dataset)))\n",
    "\n",
    "        valid_file='val_imgs.txt'\n",
    "        valid_dir='train_data'\n",
    "        valid_list = os.path.join(root_path, valid_file)\n",
    "        valid_root = os.path.join(root_path, valid_dir)\n",
    "        valid_set = Autopet(valid_list, valid_root,'valid',args.input_modality,OOD_Condition=args.OOD_Condition, level=args.OOD_Level)\n",
    "        valid_loader = DataLoader(valid_set, batch_size=1)\n",
    "        print('Samples for valid = {}'.format(len(valid_loader.dataset)))\n",
    "\n",
    "        # left just in case\n",
    "        test_file='test_imgs.txt'\n",
    "        test_dir='train_data'\n",
    "        test_list = os.path.join(root_path, test_file)\n",
    "        test_root = os.path.join(root_path, test_dir)\n",
    "        test_set = Autopet(test_list, test_root,'test',args.input_modality,OOD_Condition=args.OOD_Condition, level=args.OOD_Level)\n",
    "        test_loader = DataLoader(test_set, batch_size=1)\n",
    "        print('Samples for test = {}'.format(len(test_loader.dataset)))\n",
    "        \n",
    "    else:\n",
    "        train_loader=None\n",
    "        valid_loader=None\n",
    "        test_loader=None\n",
    "        print('There is no this dataset')\n",
    "        raise NameError\n",
    "    return train_loader,valid_loader,test_loader\n",
    "\n",
    "def val(args,model,checkpoint_dir,epoch,best_dice,valid_loader):\n",
    "\n",
    "    print('Samples for valid = {}'.format(len(valid_loader.dataset)))\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        best_dice,aver_dice,aver_iou = validate_softmax(args,save_dir = checkpoint_dir,\n",
    "                                                        best_dice = best_dice,\n",
    "                                                current_epoch = epoch,\n",
    "                                                valid_loader = valid_loader,\n",
    "                                                model = model,\n",
    "                                                names = valid_loader.dataset.image_list,\n",
    "                                                )\n",
    "        # dice_list.append(aver_dice)\n",
    "        # iou_list.append(aver_iou)\n",
    "    end_time = time.time()\n",
    "    full_test_time = (end_time-start_time)/60\n",
    "    average_time = full_test_time/len(valid_loader.dataset)\n",
    "    print('{:.2f} minutes!'.format(average_time))\n",
    "    return best_dice,aver_dice,aver_iou\n",
    "\n",
    "def train(args,model,train_loader,valid_loader,criterion_dl,model_name):\n",
    "    print('Samples for train = {}'.format(len(train_loader.dataset)))\n",
    "\n",
    "    logging.info('--------------------------------------This is all argsurations----------------------------------')\n",
    "    for arg in vars(args):\n",
    "        logging.info('{}={}'.format(arg, getattr(args, arg)))\n",
    "    logging.info('----------------------------------------This is a halving line----------------------------------')\n",
    "    logging.info('{}'.format(args.description))\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    model.cuda()\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    checkpoint_dir = os.path.join(os.path.abspath(os.getcwd()), 'checkpoint', args.experiment+args.date)\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    resume = ''\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    if os.path.isfile(resume) and args.load:\n",
    "        logging.info('loading checkpoint {}'.format(resume))\n",
    "        checkpoint = torch.load(resume, map_location=lambda storage, loc: storage)\n",
    "\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "        logging.info('Successfully loading checkpoint {} and training from epoch: {}'\n",
    "                     .format(args.resume, args.start_epoch))\n",
    "    else:\n",
    "        logging.info('re-training!!!')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    torch.set_grad_enabled(True)\n",
    "    loss_list = []\n",
    "    dice_list = []\n",
    "    iou_list = []\n",
    "    best_dice =0\n",
    "    # args.start_epoch\n",
    "    for epoch in range(args.start_epoch, args.end_epochs):\n",
    "        epoch_loss = 0\n",
    "        loss = 0\n",
    "        runtimes=[]\n",
    "        # loss1 = 0\n",
    "        # loss2 = 0\n",
    "        # loss3 = 0\n",
    "        setproctitle.setproctitle('{}: {}/{}'.format(args.user, epoch+1, args.end_epochs))\n",
    "        start_epoch = time.time()\n",
    "        for i, data in enumerate(train_loader):\n",
    "\n",
    "            adjust_learning_rate(optimizer, epoch, args.end_epochs, args.lr)\n",
    "\n",
    "            x, target = data\n",
    "            x = x.cuda()\n",
    "            target = target.cuda()\n",
    "            print('x')\n",
    "            print(x.shape)\n",
    "            print('target')\n",
    "            print(target.shape)\n",
    "\n",
    "            if model_name =='PU': # anissa: not relevant\n",
    "                if args.dataset == 'BraTS':\n",
    "                    onehot_target = one_hot_co(target, args.num_classes)\n",
    "                else:\n",
    "                    onehot_target = get_soft_label(target, args.num_classes).permute(0, 3, 1, 2)\n",
    "                # output = model(x)\n",
    "                model.forward(x, onehot_target, training=True)\n",
    "                elbo = model.elbo(onehot_target)\n",
    "                reg_loss = l2_regularisation(model.posterior) + l2_regularisation(model.prior) + l2_regularisation(\n",
    "                    model.fcomb.layers)\n",
    "                loss = -elbo + 1e-5 * reg_loss\n",
    "            else:\n",
    "                torch.cuda.synchronize()  # add the code synchronize() to correctly count the runtime.\n",
    "                start_time = time.time()\n",
    "                output = model(x)\n",
    "                torch.cuda.synchronize()\n",
    "                elapsed_time = time.time() - start_time\n",
    "                # logging.info('Single sample train time consumption {:.2f} minutes!'.format(elapsed_time / 60))\n",
    "                runtimes.append(elapsed_time)\n",
    "                # if args.dataset == 'LiTS':\n",
    "                #     target = target.unsqueeze(1)\n",
    "\n",
    "                output = F.softmax(output,1)\n",
    "                target = target.unsqueeze(1) # for SDiceloss\n",
    "                soft_target = get_soft_label(target, args.num_classes) # for SDiceloss: mean loss\n",
    "                loss = criterion_dl(output, soft_target)  # for SDiceloss\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            reduce_loss = loss.data.cpu().numpy()\n",
    "\n",
    "            logging.info('Epoch: {}_Iter:{}  loss: {:.5f}'\n",
    "                        .format(epoch, i, reduce_loss))\n",
    "            epoch_loss += reduce_loss\n",
    "\n",
    "        end_epoch = time.time()\n",
    "        loss_list.append(epoch_loss)\n",
    "\n",
    "        writer.add_scalar('lr', optimizer.defaults['lr'], epoch)\n",
    "        writer.add_scalar('loss', loss, epoch)\n",
    "\n",
    "        epoch_time_minute = (end_epoch-start_epoch)/60\n",
    "        remaining_time_hour = (args.end_epochs-epoch-1)*epoch_time_minute/60\n",
    "        logging.info('Current epoch time consumption: {:.2f} minutes!'.format(epoch_time_minute))\n",
    "        logging.info('Estimated remaining training time: {:.2f} hours!'.format(remaining_time_hour))\n",
    "        best_dice,aver_dice,aver_iou = val(args,model,checkpoint_dir,epoch,best_dice,valid_loader)\n",
    "        dice_list.append(aver_dice)\n",
    "        iou_list.append(aver_iou)\n",
    "    writer.close()\n",
    "    # validation\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = (end_time-start_time)/3600\n",
    "    logging.info('The total training time is {:.2f} hours'.format(total_time))\n",
    "    logging.info('----------------------------------The training process finished!-----------------------------------')\n",
    "\n",
    "    loss_plot(args, loss_list)\n",
    "    metrics_plot(args, 'dice',dice_list)\n",
    "\n",
    "def test(args,model,test_loader):\n",
    "    for arg in vars(args):\n",
    "        logging.info('{}={}'.format(arg, getattr(args, arg)))\n",
    "    logging.info('----------------------------------------This is a halving line----------------------------------')\n",
    "    logging.info('{}'.format(args.description))\n",
    "\n",
    "    print('Samples for test = {}'.format(len(test_loader.dataset)))\n",
    "\n",
    "    logging.info('final test........')\n",
    "    load_file = os.path.join(os.path.abspath(os.getcwd()),\n",
    "                             'checkpoint', args.experiment + args.test_date, args.model_name + '_' +args.dataset +'_'+ args.folder + '_epoch_{}.pth'.format(args.test_epoch))\n",
    "    # load_file = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n",
    "    #                          'checkpoint', args.experiment + args.test_date, args.model_name  + '_epoch_{}.pth'.format(args.test_epoch))\n",
    "\n",
    "    if os.path.exists(load_file):\n",
    "        checkpoint = torch.load(load_file)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        print('Successfully load checkpoint {}'.format(os.path.join(args.experiment + args.test_date, args.model_name + '_' + args.dataset+'_'+ args.folder  + '_epoch_{}.pth')))\n",
    "    else:\n",
    "        print('There is no resume file to load!')\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        aver_dice,aver_noise_dice,aver_hd,aver_noise_hd,aver_assd,aver_noise_assd  = test_softmax(args, test_loader = test_loader,\n",
    "                                            model = model,\n",
    "                                            load_file=load_file,\n",
    "                                            names = test_loader.dataset.image_list,\n",
    "                                            )\n",
    "    end_time = time.time()\n",
    "    full_test_time = (end_time-start_time)/60\n",
    "    average_time = full_test_time/len(test_loader.dataset)\n",
    "    print('{:.2f} minutes!'.format(average_time))\n",
    "    if args.dataset=='BraTS':\n",
    "        logging.info('aver_dice_WT=%f,aver_dice_TC = %f,aver_dice_ET = %f' % (aver_dice[0],aver_dice[1],aver_dice[2]))\n",
    "        logging.info('aver_noise_dice_WT=%f,aver_noise_dice_TC = %f,aver_noise_dice_ET = %f' % (aver_noise_dice[0], aver_noise_dice[1], aver_noise_dice[2]))\n",
    "        logging.info('aver_hd_WT=%f,aver_hd_TC = %f,aver_hd_ET = %f' % (aver_hd[0],aver_hd[1],aver_hd[2]))\n",
    "        logging.info('aver_noise_hd_WT=%f,aver_noise_hd_TC = %f,aver_noise_hd_ET = %f' % (aver_noise_hd[0], aver_noise_hd[1], aver_noise_hd[2]))\n",
    "        logging.info('aver_assd_WT=%f,aver_assd_TC = %f,aver_assd_ET = %f' % (aver_assd[0],aver_assd[1],aver_assd[2]))\n",
    "        logging.info('aver_noise_assd_WT=%f,aver_noise_assd_TC = %f,aver_noise_assd_ET = %f' % (aver_noise_assd[0], aver_noise_assd[1], aver_noise_assd[2]))\n",
    "    elif args.dataset == 'LiTS':\n",
    "        logging.info('aver_dice_Liver=%f,aver_dice_Tumor = %f' % (aver_dice[0], aver_dice[1]))\n",
    "        logging.info('aver_noise_dice_Liver=%f,aver_noise_dice_Tumor = %f' % (\n",
    "        aver_noise_dice[0], aver_noise_dice[1]))\n",
    "        logging.info('aver_hd_Liver=%f,aver_hd_Tumor = %f' % (aver_hd[0], aver_hd[1]))\n",
    "        logging.info('aver_noise_hd_Liver=%f,aver_noise_hd_Tumor = %f' % (\n",
    "        aver_noise_hd[0], aver_noise_hd[1]))\n",
    "        logging.info('aver_assd_Liver=%f,aver_assd_Tumor = %f' % (aver_assd[0], aver_assd[1]))\n",
    "        logging.info('aver_noise_assd_Liver=%f,aver_noise_assd_Tumor = %f' % (\n",
    "        aver_noise_assd[0], aver_noise_assd[1]))\n",
    "    else:\n",
    "        logging.info('aver_dice=%f' % (aver_dice))\n",
    "        logging.info('aver_noise_dice=%f' % (aver_noise_dice))\n",
    "        logging.info('aver_hd=%f' % (aver_hd))\n",
    "        logging.info('aver_noise_hd=%f' % (aver_noise_hd))\n",
    "        logging.info('aver_hd=%f' % (aver_assd))\n",
    "        logging.info('aver_noise_hd=%f' % (aver_noise_assd))\n",
    "\n",
    "def test_ensemble(args,models,test_loader):\n",
    "\n",
    "    print('Samples for test = {}'.format(len(test_loader.dataset)))\n",
    "\n",
    "    logging.info('final test........')\n",
    "\n",
    "\n",
    "    # load ensemble models\n",
    "    load_model=[]\n",
    "    # load_model[0]=.23\n",
    "    for i in range(args.en_time):\n",
    "        load_file = os.path.join(os.path.abspath(os.path.dirname(os.getcwd())),\n",
    "                             'checkpoint', args.experiment + args.test_date, args.model_name + str(i) + '_' + args.dataset+ '_' + args.folder +'_epoch_' +'199' +'.pth')\n",
    "        load_model.append(torch.load(load_file))\n",
    "        # KK =model[i]\n",
    "        models[i].load_state_dict(load_model[i]['state_dict'])\n",
    "    print('Successfully load all ensemble models')\n",
    "    start_time = time.time()\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    with torch.no_grad():\n",
    "        aver_dice,aver_noise_dice,aver_hd,aver_noise_hd,aver_assd,aver_noise_assd = testensemblemax( test_loader = test_loader,\n",
    "                                            model = models,\n",
    "                                            args=args,\n",
    "                                            names=test_loader.dataset.image_list,\n",
    "                                            )\n",
    "    end_time = time.time()\n",
    "    full_test_time = (end_time-start_time)/60\n",
    "    average_time = full_test_time/len(test_loader.dataset)\n",
    "    print('average pics {:.2f} minutes!'.format(average_time))\n",
    "    if args.dataset=='BraTS':\n",
    "        logging.info('aver_dice_WT=%f,aver_dice_TC = %f,aver_dice_ET = %f' % (aver_dice[0],aver_dice[1],aver_dice[2]))\n",
    "        logging.info('aver_noise_dice_WT=%f,aver_noise_dice_TC = %f,aver_noise_dice_ET = %f' % (aver_noise_dice[0], aver_noise_dice[1], aver_noise_dice[2]))\n",
    "        logging.info('aver_hd_WT=%f,aver_hd_TC = %f,aver_hd_ET = %f' % (aver_hd[0],aver_hd[1],aver_hd[2]))\n",
    "        logging.info('aver_noise_hd_WT=%f,aver_noise_hd_TC = %f,aver_noise_hd_ET = %f' % (aver_noise_hd[0], aver_noise_hd[1], aver_noise_hd[2]))\n",
    "        logging.info('aver_assd_WT=%f,aver_assd_TC = %f,aver_assd_ET = %f' % (aver_assd[0],aver_assd[1],aver_assd[2]))\n",
    "        logging.info('aver_noise_assd_WT=%f,aver_noise_assd_TC = %f,aver_noise_assd_ET = %f' % (aver_noise_assd[0], aver_noise_assd[1], aver_noise_assd[2]))\n",
    "    elif args.dataset=='LiTS':\n",
    "        logging.info('aver_dice_WT=%f,aver_dice_TC = %f' % (aver_dice[0],aver_dice[1]))\n",
    "        logging.info('aver_noise_dice_WT=%f,aver_noise_dice_TC = %f' % (aver_noise_dice[0], aver_noise_dice[1]))\n",
    "        logging.info('aver_hd_WT=%f,aver_hd_TC = %f' % (aver_hd[0],aver_hd[1]))\n",
    "        logging.info('aver_noise_hd_WT=%f,aver_noise_hd_TC = %f' % (aver_noise_hd[0], aver_noise_hd[1]))\n",
    "        logging.info('aver_assd_WT=%f,aver_assd_TC = %f' % (aver_assd[0],aver_assd[1]))\n",
    "        logging.info('aver_noise_assd_WT=%f,aver_noise_assd_TC = %f' % (aver_noise_assd[0], aver_noise_assd[1]))\n",
    "\n",
    "    else:\n",
    "        logging.info('aver_dice=%f' % (aver_dice))\n",
    "        logging.info('aver_noise_dice=%f' % (aver_noise_dice))\n",
    "        logging.info('aver_hd=%f' % (aver_hd))\n",
    "        logging.info('aver_noise_hd=%f' % (aver_noise_hd))\n",
    "        logging.info('aver_hd=%f' % (aver_assd))\n",
    "        logging.info('aver_noise_hd=%f' % (aver_noise_assd))\n",
    "    # logging.info('aver_dice_WT=%f,aver_dice_TC = %f,aver_dice_ET = %f' % (aver_dice[0],aver_dice[1],aver_dice[2]))\n",
    "    # logging.info('aver_noise_dice_WT=%f,aver_noise_dice_TC = %f,aver_noise_dice_ET = %f' % (aver_noise_dice[0], aver_noise_dice[1], aver_noise_dice[2]))\n",
    "    # logging.info('aver_hd_WT=%f,aver_hd_TC = %f,aver_hd_ET = %f' % (aver_hd[0],aver_hd[1],aver_hd[2]))\n",
    "    # logging.info('aver_noise_hd_WT=%f,aver_noise_hd_TC = %f,aver_noise_hd_ET = %f' % (aver_noise_hd[0], aver_noise_hd[1], aver_noise_hd[2]))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, max_epoch, init_lr, power=0.9):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = round(init_lr * np.power(1-(epoch) / max_epoch, power), 8)\n",
    "\n",
    "\n",
    "def log_args(log_file):\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s ===> %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # args FileHandler to save log file\n",
    "    fh = logging.FileHandler(log_file)\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # args StreamHandler to print log to console\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    ch.setFormatter(formatter)\n",
    "\n",
    "    # add the two Handler\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e50d753-183a-4488-9a41-ea9fedc7b31d",
   "metadata": {},
   "source": [
    "## Run pretrain UMIS in steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "460e3787-c84e-41db-91e6-4ed92b6d42a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autopet\n"
     ]
    }
   ],
   "source": [
    "args = create_args()\n",
    "#args = getArgs()\n",
    "print(args.dataset)\n",
    "if args.dataset == 'autopet':\n",
    "    args.batch_size = 2\n",
    "    args.num_classes = 2\n",
    "    args.out_size = (240, 240,160) # anissa: not sure if this is what i want?\n",
    "elif args.dataset == 'BraTS':\n",
    "    args.batch_size = 2\n",
    "    args.num_classes = 4\n",
    "    args.out_size = (240, 240,160)\n",
    "else:\n",
    "    print('There is no this dataset')\n",
    "    raise NameError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "687f29c8-76d4-43b8-a363-71f380f9b03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples for train = 737\n",
      "Samples for valid = 91\n",
      "Samples for test = 91\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader = getDataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0edf2a0b-ba1e-43fc-aa3b-959c70dc096f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/gpfs3/well/papiez/users/hri611/python/UMIS'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44d3155d-8359-4205-9097-465f48e64060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model's parameter: 4.76M\n"
     ]
    }
   ],
   "source": [
    "criterion_dl = SDiceLoss()\n",
    "    # criterion_dl = DiceLoss()\n",
    "    # criterion_dl = DC_and_BCE_loss(bce_kwargs={}, soft_dice_kwargs={})\n",
    "\n",
    "num = 2\n",
    "\n",
    "log_dir = os.path.join(os.path.abspath('/gpfs3/well/papiez/users/hri611/python/UMIS'), 'log', args.experiment + args.date)\n",
    "log_file = log_dir + '.txt'\n",
    "log_args(log_file)\n",
    "    # Net model choose\n",
    "model = getModel(args)\n",
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "print(\"Number of model's parameter: %.2fM\" % (total / 1e6))\n",
    "    # OOD_Condition = ['noise','blur','mask']\n",
    "OOD_Condition = ['mask']\n",
    "    # OOD_Condition = ['ghost','mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b60c9562-8302-4ca1-9491-e50fd0ac86b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 14:33:01 ===> --------------------------------------This is all argsurations----------------------------------\n",
      "2023-09-07 14:33:01 ===> user=anissa\n",
      "2023-09-07 14:33:01 ===> experiment=UMIS\n",
      "2023-09-07 14:33:01 ===> date=2023-09-07\n",
      "2023-09-07 14:33:01 ===> description=Trustworthy medical image segmentation by coco, training on trai_imgsn.txt!\n",
      "2023-09-07 14:33:01 ===> mode=train\n",
      "2023-09-07 14:33:01 ===> dataset=autopet\n",
      "2023-09-07 14:33:01 ===> crop_H=128\n",
      "2023-09-07 14:33:01 ===> crop_W=128\n",
      "2023-09-07 14:33:01 ===> crop_D=128\n",
      "2023-09-07 14:33:01 ===> num_classes=2\n",
      "2023-09-07 14:33:01 ===> input_modality=petct\n",
      "2023-09-07 14:33:01 ===> folder=folder0\n",
      "2023-09-07 14:33:01 ===> input_C=4\n",
      "2023-09-07 14:33:01 ===> input_H=240\n",
      "2023-09-07 14:33:01 ===> input_W=240\n",
      "2023-09-07 14:33:01 ===> input_D=160\n",
      "2023-09-07 14:33:01 ===> output_D=155\n",
      "2023-09-07 14:33:01 ===> lr=0.0001\n",
      "2023-09-07 14:33:01 ===> weight_decay=1e-05\n",
      "2023-09-07 14:33:01 ===> submission=./results\n",
      "2023-09-07 14:33:01 ===> seed=1000\n",
      "2023-09-07 14:33:01 ===> no_cuda=False\n",
      "2023-09-07 14:33:01 ===> batch_size=2\n",
      "2023-09-07 14:33:01 ===> start_epoch=0\n",
      "2023-09-07 14:33:01 ===> end_epochs=200\n",
      "2023-09-07 14:33:01 ===> save_freq=5\n",
      "2023-09-07 14:33:01 ===> resume=\n",
      "2023-09-07 14:33:01 ===> load=True\n",
      "2023-09-07 14:33:01 ===> model_name=AU\n",
      "2023-09-07 14:33:01 ===> en_time=10\n",
      "2023-09-07 14:33:01 ===> OOD_Condition=normal\n",
      "2023-09-07 14:33:01 ===> OOD_Level=1\n",
      "2023-09-07 14:33:01 ===> use_TTA=False\n",
      "2023-09-07 14:33:01 ===> snapshot=True\n",
      "2023-09-07 14:33:01 ===> save_format=nii\n",
      "2023-09-07 14:33:01 ===> test_date=2023-01-01\n",
      "2023-09-07 14:33:01 ===> test_epoch=199\n",
      "2023-09-07 14:33:01 ===> n_skip=3\n",
      "2023-09-07 14:33:01 ===> vit_name=R50-ViT-B_16\n",
      "2023-09-07 14:33:01 ===> vit_patches_size=16\n",
      "2023-09-07 14:33:01 ===> out_size=(240, 240, 160)\n",
      "2023-09-07 14:33:01 ===> ----------------------------------------This is a halving line----------------------------------\n",
      "2023-09-07 14:33:01 ===> Trustworthy medical image segmentation by coco, training on trai_imgsn.txt!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples for train = 737\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 183\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, model, train_loader, valid_loader, criterion_dl, model_name)\u001b[0m\n\u001b[1;32m    180\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------------------------------------This is a halving line----------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    181\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args\u001b[38;5;241m.\u001b[39mdescription))\n\u001b[0;32m--> 183\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed(args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m    185\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(args\u001b[38;5;241m.\u001b[39mseed)\n",
      "File \u001b[0;32m/gpfs3/well/papiez/users/hri611/python/env1-skylake/lib/python3.10/site-packages/torch/random.py:40\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[0;32m/gpfs3/well/papiez/users/hri611/python/env1-skylake/lib/python3.10/site-packages/torch/cuda/random.py:113\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    110\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    111\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 113\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs3/well/papiez/users/hri611/python/env1-skylake/lib/python3.10/site-packages/torch/cuda/__init__.py:183\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 183\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m/gpfs3/well/papiez/users/hri611/python/env1-skylake/lib/python3.10/site-packages/torch/cuda/random.py:111\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    110\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "    if 'train' in args.mode:\n",
    "        train(args,model,train_loader,valid_loader,criterion_dl,args.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aff4bc82-b013-4683-be41-9658859d17d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 44.48 GiB total capacity; 0 bytes already allocated; 1.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m foo \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m foo \u001b[38;5;241m=\u001b[39m \u001b[43mfoo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 44.48 GiB total capacity; 0 bytes already allocated; 1.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "foo = torch.tensor([1,2,3])\n",
    "foo = foo.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae704e-c6c2-48b1-93cd-93d02c398b06",
   "metadata": {},
   "source": [
    "## Make class for your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6411a82-7fab-425c-9397-a33dbd3ca79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from scipy import ndimage\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "721fd38c-1b26-4c2b-91d6-b28778963049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Crop(object):\n",
    "    def __call__(self, image,label):\n",
    "        Max_C,max_H,max_W,max_D = image.shape\n",
    "        H = random.randint(0, max_H - 128)\n",
    "        W = random.randint(0, max_W - 128)\n",
    "        D = random.randint(0, max_D - 128)\n",
    "\n",
    "        image = image[...,H: H + 128, W: W + 128, D: D + 128] \n",
    "        image = image.transpose(1, 2, 3, 0) # so it fits dimensions of code \n",
    "        \n",
    "        label = label[H: H + 128, W: W + 128, D: D + 128] # label is not in these dimensions\n",
    "        label = label.reshape(1,128,128,128) # not sure if this will work\n",
    "\n",
    "        #image = image[H: H + 128, W: W + 128, D: D + 128, ...] # might be a problem that channels are flipped in the way I do it\n",
    "        #label = label[..., H: H + 128, W: W + 128, D: D + 128]\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, image,label):\n",
    "        image = np.ascontiguousarray(image)\n",
    "        label = np.ascontiguousarray(label)\n",
    "\n",
    "        image = torch.from_numpy(image).float().unsqueeze(0)\n",
    "        label = torch.from_numpy(label).long()\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def transform(sample):\n",
    "    trans = transforms.Compose([\n",
    "        Random_Crop(),\n",
    "        ToTensor()\n",
    "    ]) # can add more data augmentation later\n",
    "\n",
    "    return trans(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2b681ee-4db2-4a41-9202-d6af164a9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autopet(Dataset):\n",
    "    def __init__(self, list_file, root='', mode='train', modal='t1',OOD_Condition = 'normal',folder='folder0',level = 0):\n",
    "        #paths, names,only_paths = [], [], []\n",
    "        names =[]\n",
    "        with open(list_file) as f:\n",
    "            for line in f:\n",
    "                # list of files will have names of all images without file ending\n",
    "                line = line.strip()\n",
    "                name = line\n",
    "                names.append(name)\n",
    "                #path = os.path.join(root, name)\n",
    "                #paths.append(path)\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.modal = modal\n",
    "        self.names = names\n",
    "        self.image_list = names\n",
    "        #self.paths = paths\n",
    "        self.OOD_Condition = OOD_Condition\n",
    "        self.folder = folder\n",
    "        self.level = level\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        name = self.names[item]\n",
    "        if self.mode in ['train']:\n",
    "            label_path = os.path.join(root,'pet_mask',name+'_mask.npy')\n",
    "            image_path = os.path.join(root,'petct',name+'_petct.npy')\n",
    "               \n",
    "            label = np.load(label_path)\n",
    "            image = np.load(image_path)\n",
    "\n",
    "            image = transform(image)\n",
    "            label = transform(label)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "        elif self.mode in ['val']:\n",
    "            label_path = os.path.join(root,'pet_mask',name+'_mask.npy')\n",
    "            image_path = os.path.join(root,'petct',name+'_petct.npy')\n",
    "               \n",
    "            label = np.load(label_path)\n",
    "            image = np.load(image_path)\n",
    "\n",
    "            image = transform(image)\n",
    "            label = transform(label)\n",
    "\n",
    "            return image, label\n",
    "\n",
    "        # need to also do same for test images / ood images\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def collate(self, batch):\n",
    "        return [torch.cat(v) for v in zip(*batch)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "214f4534-4dce-45e9-9432-2a70b14eae04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "list_train_files = [x.split(\"_\")[0]+\"_\"+x.split(\"_\")[1]+\"_\"+x.split(\"_\")[2] for x in os.listdir('/gpfs3/well/papiez/users/hri611/python/UMIS/train_data/petct')]\n",
    "     # open file in write mode\n",
    "with open(r'train_imgs.txt', 'w') as fp:\n",
    "    for item in list_train_files:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9dfecaa-032c-449b-afa9-8fcbe37a87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "# Define the paths to your text files\n",
    "train_file_path = 'train_imgs.txt'\n",
    "val_file_path = 'val_imgs.txt'\n",
    "test_file_path = 'test_imgs.txt'\n",
    "\n",
    "# Read the list of image filenames from train_imgs.txt\n",
    "with open(train_file_path, 'r') as train_file:\n",
    "    image_list = train_file.readlines()\n",
    "\n",
    "# Calculate the number of images to move to the validation set (10%)\n",
    "num_images_to_move = int(len(image_list) * 0.10)\n",
    "\n",
    "# Randomly select the images to move to the validation set\n",
    "val_images = random.sample(image_list, num_images_to_move)\n",
    "\n",
    "# Remove the selected images from the training set\n",
    "for image in val_images:\n",
    "    image_list.remove(image)\n",
    "\n",
    "# Randomly select the images to move to the validation set\n",
    "test_images = random.sample(image_list, num_images_to_move)\n",
    "# Remove the selected images from the training set\n",
    "for image in test_images:\n",
    "    image_list.remove(image)\n",
    "\n",
    "# Write the remaining images back to train_imgs.txt\n",
    "with open(train_file_path, 'w') as train_file:\n",
    "    train_file.writelines(image_list)\n",
    "\n",
    "# Write the selected images to val_imgs.txt\n",
    "with open(val_file_path, 'w') as val_file:\n",
    "    val_file.writelines(val_images)\n",
    "    \n",
    "# Write the selected images to test_imgs.txt\n",
    "with open(test_file_path, 'w') as test_file:\n",
    "    test_file.writelines(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "447d2dd8-1eac-4e7b-acde-c1fde20310fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Autopet at 0x7fe50ddf7ee0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_root = '/gpfs3/well/papiez/users/hri611/python/Evidential-neural-network-for-lymphoma-segmentation/LYMPHOMA/Data'\n",
    "train_txt = 'train_imgs.txt'\n",
    "\n",
    "Autopet(train_txt,train_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f716530a-df19-4674-bc4b-eec87b96b46d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1-skylake",
   "language": "python",
   "name": "env1-skylake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
